{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6767996f-3c99-4aac-af19-b160e712d972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: transformers[torch]\n",
      "Name: transformers\n",
      "Version: 4.46.3\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abcdef6089eb45e6a174140c43399d01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5121 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cebce322c0c04ccbb2e496c41865af1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1281 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "776cb593adf946f9891a8705ffdd3289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Model Accuracy: 0.5599422243620606\n",
      "Baseline Model F1 (Weighted): 0.40198321415622007\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74adf81c8748413abeaa98bfe383d5e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/74/f8tmrlns7r56nrvf7qwpyvh40000gn/T/ipykernel_3551/3626166399.py:103: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  pidgin_trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='23' max='963' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 23/963 57:34 < 42:56:58, 0.01 it/s, Epoch 0.07/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Installing required libraries for the project\n",
    "!pip install datasets evaluate transformers[torch] scikit-learn matplotlib seaborn -U\n",
    "\n",
    "# Importing essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import evaluate\n",
    "\n",
    "# Check Transformers library version\n",
    "!pip show transformers\n",
    "\n",
    "# Step 1: Load the Pidgin-English dataset\n",
    "# Load data from TSV files for training, development, and testing\n",
    "train_data = pd.read_csv('Data/pcm_train.tsv', sep='\\t')\n",
    "dev_data = pd.read_csv('Data/pcm_dev.tsv', sep='\\t')\n",
    "test_data = pd.read_csv('Data/pcm_test.tsv', sep='\\t')\n",
    "\n",
    "# Map sentiment labels to numeric format\n",
    "label_to_id = {'positive': 0, 'neutral': 1, 'negative': 2}\n",
    "train_data['label'] = train_data['label'].map(label_to_id)\n",
    "dev_data['label'] = dev_data['label'].map(label_to_id)\n",
    "test_data['label'] = test_data['label'].map(label_to_id)\n",
    "\n",
    "# Convert dataframes to Hugging Face Dataset format\n",
    "train_ds = Dataset.from_pandas(train_data)\n",
    "dev_ds = Dataset.from_pandas(dev_data)\n",
    "test_ds = Dataset.from_pandas(test_data)\n",
    "\n",
    "# Step 2: Initialize the BERT tokenizer\n",
    "# Using `bert-base-cased` for better handling of case-sensitive text\n",
    "pidgin_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "# Step 3: Tokenize the dataset\n",
    "def prepare_tokenized_data(examples):\n",
    "    \"\"\"\n",
    "    Tokenizes Pidgin-English text for sentiment classification.\n",
    "    Pads/truncates sequences to a fixed length of 256 tokens.\n",
    "    \"\"\"\n",
    "    return pidgin_tokenizer(examples['tweet'], padding='max_length', truncation=True, max_length=256)\n",
    "\n",
    "# Apply tokenization\n",
    "train_ds = train_ds.map(prepare_tokenized_data, batched=True)\n",
    "dev_ds = dev_ds.map(prepare_tokenized_data, batched=True)\n",
    "test_ds = test_ds.map(prepare_tokenized_data, batched=True)\n",
    "\n",
    "# Step 4: Load evaluation metrics\n",
    "f1_evaluator = evaluate.load(\"f1\", config_name='weighted')\n",
    "accuracy_evaluator = evaluate.load(\"accuracy\")\n",
    "\n",
    "# Define the metrics computation function\n",
    "def evaluate_metrics(predictions):\n",
    "    \"\"\"\n",
    "    Computes accuracy and F1 scores for sentiment classification.\n",
    "    \"\"\"\n",
    "    preds = np.argmax(predictions.predictions, axis=1)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(predictions.label_ids, preds),\n",
    "        'f1_micro': f1_score(predictions.label_ids, preds, average='micro'),\n",
    "        'f1_macro': f1_score(predictions.label_ids, preds, average='macro'),\n",
    "        'f1_weighted': f1_score(predictions.label_ids, preds, average='weighted'),\n",
    "    }\n",
    "\n",
    "# Step 5: Baseline model\n",
    "# Train a DummyClassifier for comparison\n",
    "baseline_model = DummyClassifier(strategy=\"most_frequent\")\n",
    "baseline_model.fit(train_data['tweet'], train_data['label'])\n",
    "baseline_preds = baseline_model.predict(test_data['tweet'])\n",
    "\n",
    "# Evaluate baseline performance\n",
    "baseline_accuracy = accuracy_score(test_data['label'], baseline_preds)\n",
    "baseline_f1_weighted = f1_score(test_data['label'], baseline_preds, average='weighted')\n",
    "print(\"Baseline Model Accuracy:\", baseline_accuracy)\n",
    "print(\"Baseline Model F1 (Weighted):\", baseline_f1_weighted)\n",
    "\n",
    "# Step 6: Load the BERT model\n",
    "# Using BERT instead of DistilBERT for classification\n",
    "pidgin_model = BertForSequenceClassification.from_pretrained('bert-base-cased', num_labels=3)\n",
    "\n",
    "# Step 7: Configure training arguments\n",
    "training_parameters = TrainingArguments(\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir='./training_logs',\n",
    "    logging_steps=100,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    output_dir='./model_results',\n",
    "    overwrite_output_dir=True,\n",
    ")\n",
    "\n",
    "# Step 8: Train the model\n",
    "pidgin_trainer = Trainer(\n",
    "    model=pidgin_model,\n",
    "    args=training_parameters,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=dev_ds,\n",
    "    tokenizer=pidgin_tokenizer,\n",
    "    compute_metrics=evaluate_metrics,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "pidgin_trainer.train()\n",
    "\n",
    "# Step 9: Evaluate on the test dataset\n",
    "test_results = pidgin_trainer.evaluate(test_ds)\n",
    "test_predictions = np.argmax(test_results.predictions, axis=1)\n",
    "test_labels = test_results.label_ids\n",
    "\n",
    "print(\"Test Set Accuracy:\", test_results[\"eval_accuracy\"])\n",
    "print(\"Test Set Weighted F1:\", test_results[\"eval_weighted_f1\"])\n",
    "\n",
    "# Step 10: Confusion matrix visualization\n",
    "conf_matrix = confusion_matrix(test_labels, test_predictions)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap=\"Blues\", fmt='g')\n",
    "plt.xlabel('Predicted Sentiment')\n",
    "plt.ylabel('True Sentiment')\n",
    "plt.title('Confusion Matrix for Pidgin-English Sentiment Analysis')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef5fce0-4521-4aa4-95d7-fb744062e0e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NLP)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
