{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ecac8d3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69ec9727",
   "metadata": {},
   "source": [
    "#### Description: This script is designed for performing natural language processing tasks using the DistilBert model.\n",
    "#### The script uses the Transformers, Datasets, and Accelerate libraries to facilitate model training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a17b1b",
   "metadata": {},
   "source": [
    "## 1. Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c7b2d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (3.1.0)\n",
      "Requirement already satisfied: evaluate in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (0.4.3)\n",
      "Requirement already satisfied: accelerate in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (1.0.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from datasets) (3.10.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages/huggingface_hub-0.26.2-py3.8.egg (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from accelerate) (2.2.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from aiohttp->datasets) (1.15.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from huggingface-hub>=0.23.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (1.13.2)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "zsh:1: no matches found: transformers[torch]\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (3.7.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from matplotlib) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from matplotlib) (6.4.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.20.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: seaborn in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from seaborn) (1.24.3)\n",
      "Requirement already satisfied: pandas>=1.2 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from seaborn) (2.0.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from seaborn) (3.7.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.4.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (6.4.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from pandas>=1.2->seaborn) (2023.3)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib!=3.6.1,>=3.4->seaborn) (3.20.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Name: transformers\n",
      "Version: 4.46.3\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /opt/anaconda3/envs/nlp_env/lib/python3.8/site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "# Installing necessary libraries\n",
    "!pip install datasets evaluate accelerate -U\n",
    "!pip install transformers[torch]\n",
    "!pip install scikit-learn\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "\n",
    "# Importing essential libraries for NLP tasks\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import accelerate\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Checking the version of the transformers library\n",
    "!pip show transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aae467e",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Label Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da11c6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loading training, development, and test datasets from TSV files\n",
    "# Importing pandas for data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Loading training, development, and test datasets from TSV files\n",
    "train_df = pd.read_csv('Data/pcm_train.tsv', sep='\\t')\n",
    "dev_df = pd.read_csv('Data/pcm_dev.tsv', sep='\\t')\n",
    "test_df = pd.read_csv('Data/pcm_test.tsv', sep='\\t')\n",
    "\n",
    "# Mapping textual labels to numerical format for consistency\n",
    "# 'positive': 0, 'neutral': 1, 'negative': 2\n",
    "label_mapping = {'positive': 0, 'neutral': 1, 'negative': 2}\n",
    "train_df['label'] = train_df['label'].map(label_mapping)\n",
    "dev_df['label'] = dev_df['label'].map(label_mapping)\n",
    "test_df['label'] = test_df['label'].map(label_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f167e9d",
   "metadata": {},
   "source": [
    "## 3. Dataset Conversion and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea96dded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f5b96f6810346448b048c4d60f2bd93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5121 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c955795919245ccaf1056d0decfa9b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1281 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe2544cb01841f7aaadf3c697dd5fd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Converting pandas dataframes to Hugging Face 'datasets' format\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "dev_dataset = Dataset.from_pandas(dev_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Initializing the tokenizer from the DistilBert model\n",
    "# tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n",
    "\n",
    "\n",
    "\n",
    "# Defining a function for tokenization\n",
    "def tokenize_function(examples):\n",
    "    # Tokenizing the text data with appropriate padding and truncation\n",
    "    return tokenizer(examples['tweet'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "# Applying the tokenization function to the datasets\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "dev_dataset = dev_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc8c6f4",
   "metadata": {},
   "source": [
    "## 4. Metrics Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2429cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading metrics for evaluation\n",
    "f1_metric = evaluate.load(\"f1\", config_name='weighted')\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# Defining a function to compute metrics during model evaluation\n",
    "def compute_metrics(p):\n",
    "    predictions = np.argmax(p.predictions, axis=1)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(p.label_ids, predictions),\n",
    "        'f1_micro': f1_score(p.label_ids, predictions, average='micro'),\n",
    "        'f1_macro': f1_score(p.label_ids, predictions, average='macro'),\n",
    "        'f1_weighted': f1_score(p.label_ids, predictions, average='weighted')\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d8c8ab",
   "metadata": {},
   "source": [
    "## 5. Dummy Classifier (Most-Frequent Class) Initialization and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed3984f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features (tweets) and labels from the DataFrame\n",
    "X_train = train_df['tweet']\n",
    "y_train = train_df['label']\n",
    "X_test = test_df['tweet']\n",
    "y_test = test_df['label']\n",
    "\n",
    "#Dummy Clasifier Initialization and Testing\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "y_pred = dummy_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598f1a08",
   "metadata": {},
   "source": [
    "## 6. Dummy Classifier Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "195f1854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5599422243620606\n",
      "Weighted F1 Score: 0.40198321415622007\n"
     ]
    }
   ],
   "source": [
    "# Calculate Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Calculate Weighted F1 Score\n",
    "weighted_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Weighted F1 Score:\", weighted_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36826710",
   "metadata": {},
   "source": [
    "## 7.Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecc588e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initializing the model for sequence classification\n",
    "# model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce67c07",
   "metadata": {},
   "source": [
    "## 8. Training Configuration Completion and Trainer Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a54d6d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Completing the training arguments configuration\n",
    "# training_args = TrainingArguments(\n",
    "#     per_device_train_batch_size=16,\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     num_train_epochs=4,\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     learning_rate=2e-5,\n",
    "#     weight_decay=0.01,\n",
    "#     save_strategy=\"epoch\",\n",
    "#     logging_dir='./logs',\n",
    "#     logging_steps=100,\n",
    "#     do_train=True,\n",
    "#     do_eval=True,\n",
    "#     output_dir='./results',\n",
    "#     overwrite_output_dir=True,\n",
    "#     push_to_hub=False,\n",
    "# )\n",
    "\n",
    "# # Initialize Trainer with compute metrics function\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=dev_dataset,\n",
    "#     tokenizer=tokenizer,\n",
    "#     compute_metrics=compute_metrics\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# Updated Training Configuration\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='./results',\n",
    "#     num_train_epochs=4,\n",
    "#     per_device_train_batch_size=16,\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     learning_rate=2e-5,\n",
    "#     weight_decay=0.01,\n",
    "#     eval_strategy=\"epoch\",  # Changed from evaluation_strategy\n",
    "#     save_strategy=\"epoch\",\n",
    "#     logging_dir='./logs',\n",
    "#     logging_steps=100,\n",
    "#     do_train=True,\n",
    "#     do_eval=True,\n",
    "#     overwrite_output_dir=True,\n",
    "#     push_to_hub=False,\n",
    "#     # Additional parameters for better performance\n",
    "#     gradient_accumulation_steps=2,\n",
    "#     warmup_steps=500\n",
    "# )\n",
    "\n",
    "# When using CPU\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=4,      # Smaller batch size for CPU\n",
    "    per_device_eval_batch_size=4,       # Smaller batch size for CPU\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"no\",                 # Evaluate only at end to save time\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=20,                   # More frequent updates\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    overwrite_output_dir=True,\n",
    "    push_to_hub=False,\n",
    "    gradient_accumulation_steps=8,      # Accumulate gradients\n",
    "    save_total_limit=1,                 # Save only the last model\n",
    "    dataloader_num_workers=0,           # CPU setting\n",
    "    report_to=\"none\"                    # Disable wandb reporting\n",
    ")\n",
    "\n",
    "# Updated Trainer Initialization\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")  # Removed tokenizer parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6366a8a8",
   "metadata": {},
   "source": [
    "## 9. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbc22d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 11/640 05:50 < 6:48:32, 0.03 it/s, Epoch 0.06/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initiating the training process\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf166eb",
   "metadata": {},
   "source": [
    "## 10. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6709ce23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluating the model on the development set\n",
    "# results = trainer.evaluate(dev_dataset)\n",
    "\n",
    "# Note: Uncomment the following line to evaluate on the test dataset after finalizing the model.\n",
    "results = trainer.evaluate(test_dataset)\n",
    "predicted_labels = results.predictions.argmax(-1)\n",
    "true_labels = results.label_ids\n",
    "\n",
    "# Printing evaluation results\n",
    "print(\"Weighted F1 on Test Set:\", results[\"eval_weighted_f1\"])\n",
    "print(\"Accuracy on Test Set:\", results[\"eval_accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daac064",
   "metadata": {},
   "source": [
    "## 11. Plotting Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c37a6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming you have true labels and predictions from your model\n",
    "\n",
    "# Generating the confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163d7f05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353308ea-8457-4703-a8be-79aec09109ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NLP)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
